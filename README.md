# AutomatizaciÃ³n ETL diaria con Pentaho: Carga de datos desde SFTP a SQL Server

## ğŸ“ DescripciÃ³n

Este proyecto implementa un proceso de automatizaciÃ³n diaria utilizando **Pentaho Data Integration (PDI)** para extraer, transformar y cargar datos desde archivos remotos ubicados en un servidor **SFTP** hacia una base de datos **SQL Server**.

Se trata de informaciÃ³n operativa sobre llamadas y eventos de agentes en un entorno de **call center**, consolidada en la tabla `tb_tm_agent_detail`.

## ğŸ¯ Objetivo del Proceso

- Descargar archivos diarios y retroactivos (`Cisco_Agent*.csv`) desde SFTP.
- Validar duplicidad, transformar columnas y estandarizar formato de fechas.
- Consolidar y cargar los datos a la tabla destino en SQL Server.
- Eliminar archivos descargados para evitar reprocesamiento.

---

## ğŸ§± Arquitectura del Job

El job principal en Pentaho contiene los siguientes pasos:

1. `DESCARGA_AGENTEVENTDETAIL`: transformaciÃ³n que configura y obtiene variables (fechas, ruta remota, etc.).
2. `DESCARGA_AGENTEVENTDETAIL_2`: transformaciÃ³n que descarga archivos mÃºltiples desde el SFTP.
3. `AgentEventDetail`: transformaciÃ³n principal de carga:
    - Lee los archivos `.csv` (uno por fecha).
    - Realiza un `Stream Lookup` con una tabla de control para evitar duplicados.
    - Filtra registros ya procesados.
    - Carga los datos nuevos a `tb_tm_agent_detail`.

4. `Delete files`: elimina archivos temporales descargados localmente.
5. `Success`: paso final para marcar ejecuciÃ³n correcta.


---

## ğŸ”§ Herramientas utilizadas

- ğŸ› ï¸ **Pentaho Data Integration (Spoon)**
- ğŸ§¾ **SFTP** (mÃºltiples archivos con nombres dinÃ¡micos)
- ğŸ’¾ **SQL Server**
- ğŸ—ƒï¸ Archivos CSV delimitados por coma
- âš™ï¸ Variables de entorno y mapeo de campos

---

## ğŸ“Š Detalles tÃ©cnicos

- **Nombre de archivo**: `Cisco_AgentYYYYMMDD.csv`
- **Frecuencia**: ejecuciÃ³n **diaria**
- **Ventana de datos**: cada ejecuciÃ³n carga archivos de los Ãºltimos **20 dÃ­as**
- **Tabla destino**: `dbo.tb_tm_agent_detail`
- **Transformaciones clave**:
    - Renombrado de columnas (`row Date` â†’ `row_date`, etc.)
    - Cambios regionales en fechas y horas
    - ValidaciÃ³n para evitar duplicados (por fecha y llave compuesta)
    - InserciÃ³n en SQL Server

---

## ğŸ’¡ Resultados e impacto

- âœ”ï¸ AutomatizaciÃ³n completa del proceso de carga
- âœ”ï¸ ValidaciÃ³n previa que garantiza consistencia
- âœ”ï¸ ReducciÃ³n de errores humanos
- âœ”ï¸ Proceso flexible ante nuevos archivos o cambios estructurales menores

---

## ğŸ“‚ Archivos del repositorio

| Archivo/Carpeta        | DescripciÃ³n                            |
|------------------------|----------------------------------------|
| `README.md`            | Este archivo con la documentaciÃ³n      |
| `capturas/`            | Carpeta con capturas del flujo en PDI  |
| `estructura_tabla.sql` | Estructura ficticia de la tabla destino|
| `ejemplo.csv`          | CSV de ejemplo con datos dummy         |

---

## ğŸ“Œ Notas

- Este job puede adaptarse fÃ¡cilmente a otros procesos de extracciÃ³n con diferentes estructuras.
- EstÃ¡ pensado para escalarse y manejar grandes volÃºmenes de datos operativos diarios.
